\documentclass[12pt,oneside,letterpaper]{article}
\usepackage{longtable}
\usepackage{graphicx}
%\usepackage{fullpage}

\newenvironment{packed_enumerate}{ %custom enumerate for single-spacing
\vspace{-7mm}
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}
\vspace{-8mm}}

\pagestyle{headings}
\oddsidemargin 0.25in \textwidth     6.25in \topmargin     0.4in
\textheight    8.5in

\begin{document}


\title{\bfseries Data Classifier: \\
Software Requirements Specification\\
version 1.0}

\author {
\large{Team Mark}\\
\emph{Computer Science Department}\\
\emph{California Polytechnic State University}\\
\emph{San Luis Obispo, CA USA}\\
}

\date{October 10, 2018}
\maketitle \thispagestyle{empty}


\pagebreak
\tableofcontents

\addcontentsline{toc}{section}{Revision History}

\addcontentsline{toc}{section}{Credits}

\section*{Credits}
\begin{tabular}{|l|l|p{2.5in}|l|}
\hline
\textbf{Name}&\textbf{Date}&\textbf{Role}&\textbf{Version}\\
\hline
Geraldo Macias&October 9, 2018&Lead Author of Other Nonfucntional Requirements&1.0\\
\hline
Matt Yarmolich&October 9, 2018&Lead Author of System Features&1.0\\
\hline
Spencer Schurk&October 10, 2018&Lead Author of Introduction&1.0\\
\hline
&&&\\
\hline
\end{tabular}

\section*{Revision History}
\begin{tabular}{|l|l|p{2.5in}|l|}
\hline
\textbf{Name}&\textbf{Date}&\textbf{Reason for Changes}&\textbf{Version}\\
\hline
Geraldo Macias&October 8, 2018&Completion of Other Nonfunctional Requirements&1.0\\
\hline
Spencer Schurk&October 10, 2018&Addition of my User Persona, Use Case, Functional Requirement, and Non-Functinal Requirement&1.0\\
\hline
&&&\\
\hline
&&&\\
\hline
\end{tabular}

\newpage

\section{Introduction}
\subsection{Purpose}
The purpose of this document is to present the functional and non-functional requirements of our Data Classification application version 1.0. This document also features potential user personas, and fully-dressed use cases of the product. This document serves as a source of reference and guidance during the development process.

\subsection{Document Conventions}
This document is currently not adhering to any conventions. Version 1.1 will adhere to formatting conventions and will be described in this section.

\subsection{Intended Audience and Reading Suggestions}
This document was intended for and targeted at the following parties.
\subsubsection{Developers}
Developers of this project will use this document as a reference during the development process. Specifically, the Use Cases and Functional Requirements should be referenced the most.\newline
\textit{Suggested Reading Order}\newline
\begin{packed_enumerate}
\item Overall Description
\item System Features
\item Functional Requirements
\item Non-Functional Requirements\newline
\end{packed_enumerate}
\subsubsection{Customer - Mark Logic}

\subsection{Project Scope}
Provide a short description of the software being specified and its purpose, including relevant benefits, objectives, and goals. Relate the software to corporate goals or business strategies. If a separate vision and scope document is available, refer to it rather than duplicating its contents here. An SRS that specifies the next release of an evolving product should contain its own scope statement as a subset of the long-term strategic product vision.
\subsection{References}
List any other documents or Web addresses to which this SRS refers. These may include user interface style guides, contracts, standards, system requirements specifications, use case documents, or a vision and scope document. Provide enough information so that the reader could access a copy of each reference, including title, author, version number, date, and source or location.

\section{Overall Description}
\subsection{Product Perspective}
One of the biggest unsolved roadblocks for data scientists is data sanitization and grooming. Data scientists spend excessive amounts of time preparing data before even being able to do productive work. Eliminating this downtime would allow more room for data scientists to do the meaningful work that they actually want to do. Our application aims to do just this. Through the use of machine learning techniques and a robust web interface, our application will take various ungroomed data sets as input and will convert them into cohesive, organized documents as output. We want to make the process of pre-analysis data grooming as automated and painless as possible. 
\subsection{Product Features}
The primary feature of the application is that it will be able to take data from various different "silos" and automatically make meaningful, organized categories out of them. The program will be robust enough to make sense out of data that is unconventionally or even improperly formatted. The application will also have a web interface that data scientists will be able to interact with.
\subsection{User Personas}
\subsubsection{Frank McLaughlin}
\textbf{Author}: Spencer Schurk\newline
\textbf{Age}: 24\newline
\textbf{Job}: Data Scientist\newline
\textbf{Bio}:\newline
\par Frank McLaughlin is a 24 year old data scientist working for a large e-Sports company. Frank has been interested in video games from a very young age, and this interest has helped shape his life and influence his friends. His parents never appreciated his video game obsession, but Frank didn't let that stop him.
\par Frank graduated from UC Irvine with a bachelor's degree in Computer Science. After graduating, Frank worked at a startup for a delivery app. He was assigned data analysis tasks, but they were very basic. Frank did not like this job at the startup very much, as he felt it was too fast paced with very little organization. Everbody was stepping on everyone else's feet. Eventually, it was time for Frank to find a new job.
\par Frank then found a job at a major e-Sports company. Here, he was also hired as a data analyst. He likes working at this new company a lot better, since they use better tools and he can be more efficient. However, the data sets Frank works with now are much larger. He hates manually searching through tables to find the correct data types for his tasks.
\par Luckily, Frank has heard that his e-Sports company will soon be using a new tool to automatically classify the data in these data sets. He believes this will instantly help him be more efficient, and get more data analysis done quicker. Now, Frank can go home sooner and play more video games.\newline

\includegraphics[width = \textwidth]{Dave-MattYarmolich-UserStory.png}
Written by Matt Yarmolich

\subsection{User Classes and Characteristics}
\begin{longtable}{|l|p{3.8in}|}
\hline
\textbf{User Class}&\textbf{Description}\\
\hline
Developer&The Developer will design, implement,\newline test, and maintain the application.\\
\hline
Data Scientist with programming knowledge&A Data Scientist that will require\newline advanced technical features from \newline the application.\\
\hline
Data Scientist without programming knowledge&A Data Scientist that will require\newline a simplistic interface and feature set \newline from the application.\\
\hline
\end{longtable}
\subsection{Operating Environment}
The program will exist as a web based application.
\subsection{Design and Implementation Constraints}
The back end/machine learning parts of the application will have to be written in Python. Our application also must implement a web based interface.
\subsection{User Documentation}
During implementation, we will be monitoring issues through the use of JIRA. We will likely intend to include some simple documentation for using the user interface along with the application. 
\subsection{Assumptions and Dependencies}
We are assuming that we will easily be able to integrate a database solution (i.e. MongoDB) into our application during development. In addition, we are assuming that we will be able to find a well documented machine learning technique for classifying the data categories. Finally, we are assuming that we will be able to use something like Google Firebase for our web app development.


\section{Use Cases}

\subsection{\label{Selecting the Proper Classification from the UIl}Use Case 1: Select Proper Classification Category}
This use case details the the path of the data analyst providing feedback to the machine learning algorithm written by Matt Yarmolich

\begin{longtable}{|r|p{3.8in}|}
\hline
Use Case ID:&1\\
\hline
Use Case Name:&Select Proper Classification Category\\
\hline
Created By:&Matt Yarmolich\\
\hline
Last Updated By:&Matt Yarmolich\\
\hline
Date Created:&October 9, 2018\\
\hline
Date Last Updated:&October 9, 2018\\
\hline
Actors:&Data Analyst\\
\hline
Description:&A Data Analyst accesses the data classification front end by feeding it information from an HTML-5 input tag and having the information parsed by the system. .\\
\hline
Preconditions:&
\begin{packed_enumerate}
\item the Data Analyst is logged into the system.
\item the Data Analyst is a registered user for the system.
\item the Data Analyst has a dataset that needs to be analyzed.
\end{packed_enumerate}\\
\hline
Postconditions:&
\begin{packed_enumerate}
\item the dataset has been correctly parsed by the data classifier
\item the classifier has outputted a variety of tags for verification by the Data Analyst. 
\item the Data Analyst is capable of correctly identifying the dataset
\end{packed_enumerate}\\
\hline
Normal Flow:&1.0 Select the correct category from the data classifier\\
&  %line needed for aligning enumeration 
\begin{packed_enumerate}
\item The Data Analyst uploads the dataset to the front end of the system
\item System displays upload status
\item System displays upload complete and feeds the data to the backend
\item The backend classifies the initial column data
\item The backend classifies the rest of the data based on past data columns and previous data fed through the system
\item the backend outputs the results of the data in a web-parsable format (JSON)
\item The front end parses this output and displays it to the data analyst along with the columns in question
\item the Data Analyst reviews the data and selects the correct category based on their domain knowledge
\item The front end feeds their result back to the data classification backend
\item System displays the correct tagging of the data set
\end{packed_enumerate}\\
\hline
Alternative Flows:&1.1 No results from backend (branch after step )\\
&  %line needed for aligning enumeration 
\begin{packed_enumerate}
\item The system outputs no previously learned categories
\item The Data Analyst inputs a new category for the data set
\item Return to step 8.
\end{packed_enumerate}\\
\hline
Exceptions:&1.0.E.1 System loses connection to front end(at step 1)\\
&1. 	System informs Data Analyst that connection has been lost to the server\\
&2.	Patron reloads page\\
&3.	System restarts use case.\\
\hline
Includes:&None\\
\hline
Priority:&High\\
\hline
Frequency of Use:&Approximately n uses per data set (depends on column inputted)\\
\hline
Business Rules:&TBD\\
\hline
Special Requirements:&
\begin{packed_enumerate}
\item Patron shall be able to cancel the meal order at any time prior to confirming the order.
\item Patron shall be able to view all meals he ordered within the previous six months and repeat one of those meals as the new order, provided that all food items are available on the menu for the requested delivery date. (Priority = medium)
\end{packed_enumerate}\\
\hline
Assumptions:&Assume that 30 percent of Patrons will order the daily special (source: previous six months of cafeteria data).\\
\hline
Notes and Issues:&
\begin{packed_enumerate}
\item The default date is the current date if the Patron is using the system before today's order cutoff time. Otherwise, the default date is the next day that the cafeteria is open.
\item If Patron doesn't want to have the meal delivered, the precondition requiring registration for payroll deduction is not applicable.
\item Peak usage load for this use case is between 8:00am and 10:00am local time.
\end{packed_enumerate}\\
\hline
\end{longtable}


\subsection{\label{Upload Data Sets}Use Case 2: Upload Data Sets}
This use case details the the user flow of uploading data sets to the data classifier using the web interface, written by Spencer Schurk.

\begin{longtable}{|r|p{3.8in}|}
\hline
Use Case ID:&2\\
\hline
Use Case Name:&Upload Data Sets\\
\hline
Created By:&Spencer Schurk\\
\hline
Last Upadted By:&Spencer Schurk\\
\hline
Date Created:&October 10, 2018\\
\hline
Date Last Updated:&October 10, 2018\\
\hline
Actors:&Data Analyst\\
\hline
Description:&After logging in, the system allows a data analyst to upload multiple data sets. After uploading data sets, the data analyst can begin the classification process.\\
\hline
Preconditions:&\begin{packed_enumerate}
\item The data analyst is logged into the system.
\item The data analyst has not uploaded any data sets.
\item The data analyst has not began the classification process.
\end{packed_enumerate}\\
\hline
Postconditions:&\begin{packed_enumerate}
\item All selected data sets have been uploaded.
\item The data analyst is now able to begin the classification process.
\end{packed_enumerate}\\
\hline
Normal Flow:&1.0 Select multiple files to upload.\newline
\begin{packed_enumerate}
\item The data analyst selects "Upload Files" button.
\item System opens local machine's file explorer / file selector.
\item Data analyst selects multiple files from their local machine.
\item Data analyst selects "Ok" or "Upload" on system file explorer.
\item System displays list of files selected and begins to upload files.
\item System displays progress of each file being uploaded.
\item System displays "Upload successful" message after upload completes.
\item Data analyst selects "Begin Classification" button.
\end{packed_enumerate}\\
\hline
Alternate Flows:&1.1 Select files more than once. (branch after step 4)\newline
\begin{packed_enumerate}
\item While previously selected files are uploading, data analyst selects "Upload more files"
\item System opens local machine's file explorer / file selector.
\item Data analyst selects multiple files from their local machine.
\item Data analyst selects "Ok" or "Upload" on system file explorer.
\item System adds selected files to list of previously selected files that are uploading.
\item Flow continues on step 8 of normal flow
\end{packed_enumerate}\\
\hline
Exceptions:&1.0.E.1 Data analyst uploads unsupported file. (at step 4)\newline
\begin{packed_enumerate}
\item System displays error message, with file name present.
\item Data analyst selects "OK" button.
\item System removes selected file from list of files to upload.
\item System continues to upload other selected files.\newline
\end{packed_enumerate}
1.0.E.2 Upload fails for any reason. (at step 5)\newline
\begin{packed_enumerate}
\item System displays "Upload failed" message
\item Data analyst selects "Try again" or "Cancel" button.
\item If "Try again" is selected, system continues at step 5, with all progress at 0.
\item If "Cancel" button selected, page is refreshed, and continues at step 1.
\end{packed_enumerate}\\
\hline
Includes:&None\\
\hline
Priority&High\\
\hline
Frequency of Use:&Once per data classification process.\\
\hline
Business Rules:&TBD\\
\hline
Special Requirements:&TBD\\
\hline
Assumptions:&\begin{packed_enumerate}
\item Data sets are locally stored on data analyst's machine.
\item Data analyst has active internet connection capable of uploading files.
\end{packed_enumerate}\\
\hline
Notes and Issues:&\begin{packed_enumerate}
\item What if the data analyst uploads the wrong file?
\end{packed_enumerate}\\
\hline
\end{longtable}



\subsection{\label{Whatever the next one is}Use Case 3: Whatever the next one is}
This is a casual use-case.  Note that there is a label in the LaTeX so you can refer
to it as being in section~\ref{Whatever the next one is} on page~\pageref{Whatever the next one is}.
\begin{enumerate}
\item This is step one.
\item This is step two.
\item This is step three.
\end{enumerate}

\section{System Features}
This system will be primarily used for data classification and adding meta-tags/titles to unidentified datasets inputted by a user from various formats including CSVs, JSON, and SQL. The main purpose of this product is to give Data Scientists, Data Engineers, and Data Analysis's insight on what kind of data they are collecting, and the ability to verify or modify any tags generated by the machine learning processes in our backend. This will be done through a front end written for the web interfacing with most modern browsers with javascript enabled. This front end will be powered by a backend written in Python leveraging a machine learning library such as TensorFlow. This backend will take data in from a recognized data format, parse the data into a data structure for temporary storage, feed it into this library and spit out possible classification tags. These tags will then be sent to the front end for verification by the data analyst for verification/modification. This data analyst will be able to see the data in question in order to judge the classifiers accuracy and to teach it about new datasets/data types by piping these recommendations back to the machine learning algorithm.

  \subsection{Machine Learning Python Backend}
\subsubsection{Description and Priority}
This feature of the product will allow data being fed into the product to produce meaningful data for output/viewing by the data analyst. The benefit of this algorithm is that it will provide data to the front end for easy viewing and provide an endpoint for where feedback from the data analysts. In terms of cost, this is probably going to be a fairly expensive feature to implement as its the main focus of this project and without it, the rest of the project is useless. However there are a ton of resources for implementing this feature minimizing the potential development time required. Finally, this system requirement is of High priority due to the fact it is the main feature that makes our product useful. 
\subsubsection{Stimulus/Response Sequences}
The system shall take in a dataset from the front end and then feed this dataset into the machine learning algorithm column line by line. This step will be done by the data parser which will read in the columns line by line and separate them (detailed below). This data will then be compared to previous inputs taught by the algorithm by the Software Engineers developing the program. With these comparisons, the algorithm shall develop a result that fits the dataset, or output unknown if it doesnt have any idea what the dataset is of. It also will look to previous classifications contained by the dataset 
\subsubsection{Functional Requirements}
Itemize the detailed functional requirements associated with this feature. These are the software capabilities that must be present in order for the user to carry out the services provided by the feature, or to execute the use case. Include how the product should respond to anticipated error conditions or invalid inputs. Requirements should be concise, complete, unambiguous, verifiable, and necessary. Use "TBD" as a placeholder to indicate when necessary information is not yet available.\\
\\
Each requirement should be uniquely identified with a sequence number or a meaningful tag of some kind.

\begin{enumerate}
\item REQ-1: The system shall be able to take in a specific data column and be able to identify it based on previous machine learning techniques used on the system and output a string of what it thinks it should output .
\item REQ-2: The system shall be able to learn new classifications given to it by the Data Analysts as feedback from the front end.
\item REQ-3: The system shall be able to learn categories over time as training is provided to it by the Software Engineers developing the system, and by data analysts feeding it new categories/training.
\end{enumerate} (Written by Matt Yarmolich)

\subsection{Data Classification Front End}
\subsubsection{Description and Priority}
This feature set will allow the data analyst to interact with the output of the machine learning algorithm for further classification and viewing of what the algorithm outputs. This feature set is another high priority feature as it allows our algorithm to be interacted with by the client and provide meaningful responses to help further teach the algorithm about new datasets and expectations. 
\subsubsection{Stimulus/Response Sequences}
List the sequences of user actions and system responses that stimulate the behavior defined for this feature. These will correspond to the dialog elements associated with use cases.
\subsubsection{Functional Requirements}
\begin{enumerate}
\item REQ-1: WIP
\item REQ-2: WIP
\item REQ-3: WIP
\end{enumerate}

\subsection{Data Parser}
\subsubsection{Description and Priority}
This feature set will allow the data analyst to submit data to the System and parse it into a computer readable interface and parse it into a data structure for storage and analysis. \subsubsection{Stimulus/Response Sequences}
The data analyst will select a data set to upload to the server. The server will then accept this file and parse the file. During parsing, the computer will store each columns data in separate data structures for analysis. 
\subsubsection{Functional Requirements}
\begin{enumerate}
\item REQ-1: The system shall accept data to be uploaded to the backend
\item REQ-2: The system shall parse the data, storing each row's data for each column in a separate dynamically growing
\item REQ-3: The system shall feed the data to the machine learning algorithm
\item REQ-4: The system shall be able to accept over 10 data sets to upload at a time. (Written by Spencer Schurk)
\end{enumerate}

\section{External Interface Requirements}
\subsection{User Interfaces}
Describe the logical characteristics of each interface between the software product and the users. This may include sample screen images, any GUI standards or product family style guides that are to be followed, screen layout constraints, standard buttons and functions (e.g., help) that will appear on every screen, keyboard shortcuts, error message display standards, and so on. Define the software components for which a user interface is needed. Details of the user interface design should be documented in a separate user interface specification.
\subsection{Hardware Interfaces}
Describe the logical and physical characteristics of each interface between the software product and the hardware components of the system. This may include the supported device types, the nature of the data and control interactions between the software and the hardware, and communication protocols to be used.
\subsection{Software Interfaces}
Describe the connections between this product and other specific software components (name and version), including databases, operating systems, tools, libraries, and integrated commercial components. Identify the data items or messages coming into the system and going out and describe the purpose of each. Describe the services needed and the nature of communications. Refer to documents that describe detailed application programming interface protocols. Identify data that will be shared across software components. If the data sharing mechanism must be implemented in a specific way (for example, use of a global data area in a multitasking operating system), specify this as an implementation constraint.
\subsection{Communications Interfaces}
Describe the requirements associated with any communications functions required by this product, including e-mail, web browser, network server communications protocols, electronic forms, and so on. Define any pertinent message formatting. Identify any communication standards that will be used, such as FTP or HTTP. Specify any communication security or encryption issues, data transfer rates, and synchronization mechanisms.

\section{Other Nonfunctional Requirements}
\subsection{Performance Requirements}
The customer has specified that performance is not a concern.
\begin{enumerate}
    \item The software will be usable by Data Analyst with and without programming assignments
    \item The system shall be able to be simple to understand and usable by anyone within the company that wants to use it (Written by Matt Yarmolich)
\end{enumerate}
\subsection{Safety Requirements}
During the classification stage, safety is not a concern. After the catalog phase we will begin redacting sensitive information.
\subsection{Security Requirements}
\begin{enumerate}
    \item The software will have two factor authentication and will only allow verified users to access the systems databases.
    \item The system shall not parse any uploaded data set until the user has selected "Begin Classification." (Written by Spencer Schurk)
\end{enumerate}
\subsection{Software Quality Attributes}
\begin{enumerate}
    \item The system shall have a learning curve of 24 hours. This applies to all users ranging from a sports analyst to a data scientist.
    \item The system shall be available 99.99\% of the time.
    \item The system shall be able to maintain one million datasets.
    \item The system will be portable so different companies can use our solution.
\end{enumerate}


\section{Other Requirements}
Define any other requirements not covered elsewhere in the SRS. This might include database requirements, internationalization requirements, legal requirements, reuse objectives for the project, and so on. Add any new sections that are pertinent to the project.

\appendix
\section{Glossary}
Define all the terms necessary to properly interpret the SRS, including acronyms and abbreviations. You may wish to build a separate glossary that spans multiple projects or the entire organization, and just include terms specific to a single project in each SRS.

\section{Analysis Models}
Optionally, include any pertinent analysis models, such as data flow diagrams, class diagrams, state-transition diagrams, or entity-relationship diagrams.

\section{Issues List}
This is a dynamic list of the open requirements issues that remain to be resolved, including TBDs, pending decisions, information that is needed, conflicts awaiting resolution, and the like.
\end{document}
